import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime
from app.db import get_db
from app.model.models import Article, User, CrawlTask
from app.services.services import generate_all_content, get_openai_client
from app.core.config import settings
import os
import threading
import time

def get_nhk_easy_news():
    """è·å–NHK Easyæ–°é—» - ä½¿ç”¨çœŸå®JSON API"""
    try:
        # NHK Easy JSON API
        url = "https://www3.nhk.or.jp/news/easy/top-list.json"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        response.encoding = 'utf-8'
        
        # è§£æJSONæ•°æ®
        news_data = response.json()
        
        news_list = []
        
        # å–å‰1æ¡æ–°é—»ï¼ˆç”¨æˆ·è¦æ±‚åªçˆ¬ç¬¬ä¸€æ¡ï¼‰
        for item in news_data[:1]:
            try:
                news_id = item.get('news_id', '')
                title = item.get('title', '')
                title_with_ruby = item.get('title_with_ruby', '')
                
                # æ„å»ºå®Œæ•´URL
                full_url = f"https://www3.nhk.or.jp/news/easy/{news_id}/{news_id}.html"
                
                # æå–æ¦‚è¦æ–‡æœ¬ï¼ˆç§»é™¤HTMLæ ‡ç­¾ï¼‰
                outline_with_ruby = item.get('outline_with_ruby', '')
                soup = BeautifulSoup(outline_with_ruby, 'html.parser')
                outline_text = soup.get_text()
                
                news_list.append({
                    'title': title,
                    'title_with_ruby': title_with_ruby,
                    'url': full_url,
                    'news_id': news_id,
                    'outline': outline_text,
                    'source_url': full_url  # æ·»åŠ æºURLç”¨äºè¿½è¸ª
                })
                
            except Exception as e:
                print(f"è§£ææ–°é—»é¡¹å¤±è´¥: {e}")
                continue
        
        print(f"æˆåŠŸè·å– {len(news_list)} æ¡NHK Easyæ–°é—»")
        return news_list
        
    except Exception as e:
        print(f"è·å–NHK Easyæ–°é—»å¤±è´¥: {e}")
        # è¿”å›ç¤ºä¾‹æ•°æ®ä½œä¸ºfallback
        return [
            {
                'title': 'å°é¢¨15å·ã§é›¨ã‚„é¢¨ã®è¢«å®³ã€€ã“ã‚Œã‹ã‚‰ã‚‚æ°—ã‚’ã¤ã‘ã¦',
                'url': 'https://www3.nhk.or.jp/news/easy/ne2025090511573/ne2025090511573.html',
                'source_url': 'https://www3.nhk.or.jp/news/easy/ne2025090511573/ne2025090511573.html'
            },
            {
                'title': 'æ—¥æœ¬ãŒã‚¢ãƒ¡ãƒªã‚«ã«è¼¸å‡ºã™ã‚‹è»Šãªã©ã®é–¢ç¨ã€€15%ã«ãªã‚‹',
                'url': 'https://www3.nhk.or.jp/news/easy/ne2025090511472/ne2025090511472.html',
                'source_url': 'https://www3.nhk.or.jp/news/easy/ne2025090511472/ne2025090511472.html'
            },
            {
                'title': 'ä¸–ç•Œã§æœ‰åãªãƒ‡ã‚¶ã‚¤ãƒŠãƒ¼ã€€ã‚¢ãƒ«ãƒãƒ¼ãƒ‹ã•ã‚“ãŒäº¡ããªã£ãŸ',
                'url': 'https://www3.nhk.or.jp/news/easy/ne2025090516408/ne2025090516408.html',
                'source_url': 'https://www3.nhk.or.jp/news/easy/ne2025090516408/ne2025090516408.html'
            },
            {
                'title': 'ç§‹ã®é­šã€Œã•ã‚“ã¾ã€ã€€ä»Šå¹´ã¯ãŸãã•ã‚“ã¨ã‚Œãã†ã ',
                'url': 'https://www3.nhk.or.jp/news/easy/ne2025090511546/ne2025090511546.html',
                'source_url': 'https://www3.nhk.or.jp/news/easy/ne2025090511546/ne2025090511546.html'
            },
            {
                'title': 'å°é¢¨15å·ãŒä¹å·ã®è¿‘ãã‚’é€²ã‚“ã§ã„ã‚‹ã€€é›¨ã«æ°—ã‚’ã¤ã‘ã¦',
                'url': 'https://www3.nhk.or.jp/news/easy/ne2025090412073/ne2025090412073.html',
                'source_url': 'https://www3.nhk.or.jp/news/easy/ne2025090412073/ne2025090412073.html'
            }
        ]

def get_houkago_news():
    """è·å–æ”¾èª²å¾ŒNEWS - æš‚æ—¶è¿”å›ç©ºåˆ—è¡¨ï¼ˆç½‘ç«™å¯èƒ½å·²å˜æ›´ï¼‰"""
    try:
        # æ”¾èª²å¾ŒNEWSç½‘ç«™å¯èƒ½å·²ä¸å­˜åœ¨æˆ–URLå˜æ›´ï¼Œæš‚æ—¶è¿”å›ç©ºåˆ—è¡¨
        print("æ”¾èª²å¾ŒNEWSç½‘ç«™ä¸å¯è®¿é—®ï¼Œè·³è¿‡æ­¤éƒ¨åˆ†")
        return []
    except Exception as e:
        print(f"è·å–æ”¾èª²å¾ŒNEWSå¤±è´¥: {e}")
        return []
        print(f"è·å–æ”¾èª²å¾ŒNEWSå¤±è´¥: {e}")
        # è¿”å›ç©ºåˆ—è¡¨ï¼Œè®©è°ƒç”¨æ–¹çŸ¥é“çˆ¬å–å¤±è´¥
        return []

def get_article_content(url):
    """è·å–æ–‡ç« å†…å®¹ - çœŸå®çˆ¬å–"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # NHK Easyæ–°é—»é¡µé¢çš„å†…å®¹é€šå¸¸åœ¨ç‰¹å®šçš„ç»“æ„ä¸­
        content_selectors = [
            'div.article-main__body',  # NHK Easyçš„ä¸»è¦å†…å®¹åŒºåŸŸ
            'div.article-body',
            'div.content-body',
            'div.main-text',
            'article .body',
            '.article-content'
        ]
        
        content = ""
        for selector in content_selectors:
            content_elem = soup.find(selector)
            if content_elem:
                # ç§»é™¤è„šæœ¬å’Œæ ·å¼å…ƒç´ 
                for script in content_elem.find_all(['script', 'style']):
                    script.decompose()
                
                content = content_elem.get_text(separator='\n', strip=True)
                if content and len(content) > 50:  # ç¡®ä¿å†…å®¹è¶³å¤Ÿé•¿
                    break
        
        # å¦‚æœæ²¡æ‰¾åˆ°ç‰¹å®šå†…å®¹åŒºåŸŸï¼Œå°è¯•æå–é¡µé¢ä¸»è¦æ–‡æœ¬
        if not content or len(content) < 50:
            # ç§»é™¤å¯¼èˆªã€é¡µè„šç­‰æ— å…³å…ƒç´ 
            for tag in soup.find_all(['nav', 'footer', 'header', 'aside', 'script', 'style']):
                tag.decompose()
            
            # æå–ä¸»è¦å†…å®¹
            main_content = soup.find('main') or soup.find('body')
            if main_content:
                content = main_content.get_text(separator='\n', strip=True)
        
        # æ¸…ç†å†…å®¹
        if content:
            # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
            import re
            content = re.sub(r'\n+', '\n', content)
            content = re.sub(r'\s+', ' ', content)
            content = content.strip()
            
            # ç¡®ä¿å†…å®¹ä¸ä¸ºç©ºä¸”æœ‰æ„ä¹‰
            if len(content) > 20:
                return content
        
        # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œè¿”å›æç¤ºä¿¡æ¯
        return f"æ— æ³•æå–æ–‡ç« å†…å®¹ï¼Œè¯·è®¿é—®åŸæ–‡ï¼š{url}"
        
    except Exception as e:
        print(f"è·å–æ–‡ç« å†…å®¹å¤±è´¥ {url}: {e}")
        return f"è·å–å†…å®¹å¤±è´¥ï¼š{str(e)}"

def generate_simplified_article(original_text, user_level, model, client):
    levels = {
        1: "JLPT N5æ°´å¹³ï¼ˆåŸºç¡€è¯æ±‡å’Œè¯­æ³•ï¼‰", 
        2: "JLPT N4æ°´å¹³ï¼ˆæ—¥å¸¸ä¼šè¯ï¼‰", 
        3: "JLPT N3æ°´å¹³ï¼ˆä¸€èˆ¬æ€§è¯é¢˜ï¼‰", 
        4: "JLPT N2æ°´å¹³ï¼ˆæŠ½è±¡è¯é¢˜ï¼‰", 
        5: "JLPT N1æ°´å¹³ï¼ˆå¤æ‚è¯é¢˜ï¼‰"
    }
    level_desc = levels.get(user_level, "JLPT N3æ°´å¹³ï¼ˆä¸€èˆ¬æ€§è¯é¢˜ï¼‰")
    prompt = f"è¯·å°†ä»¥ä¸‹æ—¥æ–‡æ–‡ç« ç®€åŒ–åˆ°é€‚åˆ{level_desc}çš„å­¦ä¹ è€…é˜…è¯»æ°´å¹³ã€‚ä¿æŒä¸»è¦å†…å®¹ï¼Œä½†ä½¿ç”¨ç›¸åº”ç­‰çº§çš„è¯æ±‡å’Œå¥å­ç»“æ„ï¼Œåªè¾“å‡ºç»“æœï¼Œä¸è¦è¯´æ— å…³çš„è¯ã€‚\n\nåŸæ–‡ï¼š{original_text}"
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content

def crawl_and_save_articles_background(user_id, task_id):
    """åå°å¤„ç†çˆ¬è™«ä»»åŠ¡"""
    db = next(get_db())
    
    try:
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤„ç†ä¸­
        task = db.query(CrawlTask).filter(CrawlTask.id == task_id).first()
        if not task:
            return
        
        task.status = "processing"
        task.updated_at = datetime.utcnow()
        db.commit()
        
        user = db.query(User).filter(User.id == user_id).first()
        if not user:
            task.status = "failed"
            db.commit()
            return
        
        # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦é…ç½®äº†AIè®¾ç½®
        if not user.openai_api_key:
            task.status = "failed"
            db.commit()
            return
        
        user_level = user.level
        client = get_openai_client(user.openai_api_key, user.openai_base_url)
        model = user.openai_model
        
        nhk_news = get_nhk_easy_news()
        # åªä½¿ç”¨NHK Easyæ–°é—»ï¼Œç§»é™¤æ”¾èª²å¾ŒNEWS
        all_news = nhk_news
        
        task.total_articles = len(all_news)  # ç°åœ¨æ˜¯1æ¡æ–°é—»
        
        processed_count = 0
        
        for news in all_news:
            try:
                content = get_article_content(news['url'])
                if content:
                    # æ·»åŠ æº¯æºä¿¡æ¯åˆ°å†…å®¹ç¬¬ä¸€è¡Œ
                    source_content = f"æ¥æº: {news['url']}\n\n{content}"
                    
                    # ç”Ÿæˆç®€åŒ–ç‰ˆ
                    simplified = generate_simplified_article(content, user_level, model, client)
                    # ä½¿ç”¨servicesç”Ÿæˆå®Œæ•´å†…å®¹
                    ruby_text, vocab, translation, title, emoji = generate_all_content(simplified, model, client)
                    
                    article = Article(
                        user_id=user_id,
                        title=title,
                        emoji_cover=emoji,
                        original=source_content,  # åŒ…å«æº¯æºä¿¡æ¯çš„å†…å®¹
                        ruby_html=ruby_text,
                        translation=translation,
                        vocab_json=json.dumps(vocab, ensure_ascii=False),
                        source_url=news['url']  # ä¿å­˜æºURL
                    )
                    db.add(article)
                    
                    processed_count += 1
                    task.processed_articles = processed_count
                    task.updated_at = datetime.utcnow()
                    db.commit()
                    print(f"âœ… å·²å¤„ç† {processed_count}/{task.total_articles} ç¯‡æ–‡ç« : {news['title']}")
                    
            except Exception as e:
                print(f"âŒ å¤„ç†æ–‡ç« å¤±è´¥: {news['title']}, é”™è¯¯: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
        task.status = "completed"
        task.updated_at = datetime.utcnow()
        db.commit()
        print(f"ğŸ‰ çˆ¬è™«ä»»åŠ¡å®Œæˆï¼å…±å¤„ç† {processed_count} ç¯‡æ–‡ç« ")
        
    except Exception as e:
        print(f"âŒ åå°å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        if task:
            task.status = "failed"
            task.updated_at = datetime.utcnow()
            db.commit()
    finally:
        db.close()


def crawl_and_save_articles(user_id):
    """å¯åŠ¨åå°çˆ¬è™«ä»»åŠ¡"""
    db = next(get_db())
    
    try:
        user = db.query(User).filter(User.id == user_id).first()
        if not user:
            return {"success": False, "message": "User not found"}
        
        # åˆ›å»ºæ–°çš„çˆ¬è™«ä»»åŠ¡
        task = CrawlTask(
            user_id=user_id,
            status="pending",
            total_articles=0,
            processed_articles=0
        )
        db.add(task)
        db.commit()
        db.refresh(task)
        
        # å¯åŠ¨åå°çº¿ç¨‹å¤„ç†
        thread = threading.Thread(
            target=crawl_and_save_articles_background,
            args=(user_id, task.id)
        )
        thread.daemon = True
        thread.start()
        
        return {
            "success": True, 
            "message": "çˆ¬è™«ä»»åŠ¡å·²å¯åŠ¨ï¼Œåå°å¤„ç†ä¸­",
            "task_id": task.id
        }
        
    except Exception as e:
        db.rollback()
        return {"success": False, "message": f"å¯åŠ¨å¤±è´¥: {str(e)}"}
    finally:
        db.close()
